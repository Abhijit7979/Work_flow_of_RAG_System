{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471df209",
   "metadata": {},
   "source": [
    "### Chroma \n",
    "Chroma is a Al-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4a1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build a simple vectordb\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from rich import print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba78be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Numerous\\nefforts have since continued to push the boundaries of recurrent language models and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditional\\ncomputation [32], while also improving model performance in case of the latter. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">building\\nblock, computing hidden representations in parallel for all input and output positions. In these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transduction model relying\\nentirely on self-attention to compute representations of its input and output without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as additional input when generating the next.\\n2'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the \u001b[0m\n",
       "\u001b[32mtables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You \u001b[0m\n",
       "\u001b[32mNeed\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki \u001b[0m\n",
       "\u001b[32mParmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle\u001b[0m\n",
       "\u001b[32mResearch\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle \u001b[0m\n",
       "\u001b[32mBrain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence \u001b[0m\n",
       "\u001b[32mtransduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a \u001b[0m\n",
       "\u001b[32mdecoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We \u001b[0m\n",
       "\u001b[32mpropose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with \u001b[0m\n",
       "\u001b[32mrecurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe \u001b[0m\n",
       "\u001b[32msuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model \u001b[0m\n",
       "\u001b[32machieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, \u001b[0m\n",
       "\u001b[32mincluding\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a \u001b[0m\n",
       "\u001b[32mnew single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction \u001b[0m\n",
       "\u001b[32mof the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well \u001b[0m\n",
       "\u001b[32mto\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training \u001b[0m\n",
       "\u001b[32mdata.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and \u001b[0m\n",
       "\u001b[32mstarted\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer \u001b[0m\n",
       "\u001b[32mmodels and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, \u001b[0m\n",
       "\u001b[32mmulti-head\\nattention and the parameter-free position representation and became the other person involved in nearly\u001b[0m\n",
       "\u001b[32mevery\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase \u001b[0m\n",
       "\u001b[32mand\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, \u001b[0m\n",
       "\u001b[32mand\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of \u001b[0m\n",
       "\u001b[32mand\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively \u001b[0m\n",
       "\u001b[32maccelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google \u001b[0m\n",
       "\u001b[32mResearch.\\n31st Conference on Neural Information Processing Systems \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNIPS 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Long Beach, CA, \u001b[0m\n",
       "\u001b[32mUSA.\\narXiv:1706.03762v7  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  2 Aug 2023'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'1 Introduction\\nRecurrent neural networks, long short-term memory \u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and gated recurrent \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m neural networks\\nin particular, have been firmly established as state of the art approaches in sequence \u001b[0m\n",
       "\u001b[32mmodeling and\\ntransduction problems such as language modeling and machine translation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 35, 2, 5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mNumerous\\nefforts have since continued to push the boundaries of recurrent language models and \u001b[0m\n",
       "\u001b[32mencoder-decoder\\narchitectures \u001b[0m\u001b[32m[\u001b[0m\u001b[32m38, 24, 15\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nRecurrent models typically factor computation along the symbol \u001b[0m\n",
       "\u001b[32mpositions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a \u001b[0m\n",
       "\u001b[32msequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This \u001b[0m\n",
       "\u001b[32minherently\\nsequential nature precludes parallelization within training examples, which becomes critical at \u001b[0m\n",
       "\u001b[32mlonger\\nsequence lengths, as memory constraints limit batching across examples. Recent work has \u001b[0m\n",
       "\u001b[32machieved\\nsignificant improvements in computational efficiency through factorization tricks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mconditional\\ncomputation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m32\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, while also improving model performance in case of the latter. The \u001b[0m\n",
       "\u001b[32mfundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral \u001b[0m\n",
       "\u001b[32mpart of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies\u001b[0m\n",
       "\u001b[32mwithout regard to their distance in\\nthe input or output sequences \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2, 19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In all but a few cases \u001b[0m\u001b[32m[\u001b[0m\u001b[32m27\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, however, \u001b[0m\n",
       "\u001b[32msuch attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the \u001b[0m\n",
       "\u001b[32mTransformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to \u001b[0m\n",
       "\u001b[32mdraw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization \u001b[0m\n",
       "\u001b[32mand can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on \u001b[0m\n",
       "\u001b[32meight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the \u001b[0m\n",
       "\u001b[32mExtended Neural GPU\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use convolutional neural networks as basic \u001b[0m\n",
       "\u001b[32mbuilding\\nblock, computing hidden representations in parallel for all input and output positions. In these \u001b[0m\n",
       "\u001b[32mmodels,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin\u001b[0m\n",
       "\u001b[32mthe distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult\u001b[0m\n",
       "\u001b[32mto learn dependencies between distant positions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In the Transformer this is\\nreduced to a constant number of \u001b[0m\n",
       "\u001b[32moperations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an \u001b[0m\n",
       "\u001b[32meffect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called \u001b[0m\n",
       "\u001b[32mintra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a \u001b[0m\n",
       "\u001b[32mrepresentation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading \u001b[0m\n",
       "\u001b[32mcomprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence \u001b[0m\n",
       "\u001b[32mrepresentations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead \u001b[0m\n",
       "\u001b[32mof sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering \u001b[0m\n",
       "\u001b[32mand\\nlanguage modeling tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nTo the best of our knowledge, however, the Transformer is the first \u001b[0m\n",
       "\u001b[32mtransduction model relying\\nentirely on self-attention to compute representations of its input and output without \u001b[0m\n",
       "\u001b[32musing sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, \u001b[0m\n",
       "\u001b[32mmotivate\\nself-attention and discuss its advantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n3 Model \u001b[0m\n",
       "\u001b[32mArchitecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5, 2, \u001b[0m\n",
       "\u001b[32m35\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nHere, the encoder maps an input sequence of symbol representations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to a sequence\\nof continuous\u001b[0m\n",
       "\u001b[32mrepresentations z = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., zn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Given z, the decoder then generates an output\\nsequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32my1, ..., ym\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of symbols\u001b[0m\n",
       "\u001b[32mone element at a time. At each step the model is auto-regressive\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, consuming the previously generated symbols \u001b[0m\n",
       "\u001b[32mas additional input when generating the next.\\n2'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader=PyPDFLoader(\"../3.2-dataIngestion/attention.pdf\")\n",
    "pdf=loader.load()\n",
    "print(pdf[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d642ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models and'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention, multi-head\\nattention and the parameter-free position representation and became the other person </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the \u001b[0m\n",
       "\u001b[32mtables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You \u001b[0m\n",
       "\u001b[32mNeed\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki \u001b[0m\n",
       "\u001b[32mParmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle\u001b[0m\n",
       "\u001b[32mResearch\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle \u001b[0m\n",
       "\u001b[32mBrain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence \u001b[0m\n",
       "\u001b[32mtransduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a \u001b[0m\n",
       "\u001b[32mdecoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We \u001b[0m\n",
       "\u001b[32mpropose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with \u001b[0m\n",
       "\u001b[32mrecurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe \u001b[0m\n",
       "\u001b[32msuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model \u001b[0m\n",
       "\u001b[32machieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, \u001b[0m\n",
       "\u001b[32mincluding\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a \u001b[0m\n",
       "\u001b[32mnew single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction \u001b[0m\n",
       "\u001b[32mof the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well \u001b[0m\n",
       "\u001b[32mto\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training \u001b[0m\n",
       "\u001b[32mdata.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and \u001b[0m\n",
       "\u001b[32mstarted\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer \u001b[0m\n",
       "\u001b[32mmodels and'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first \u001b[0m\n",
       "\u001b[32mTransformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product \u001b[0m\n",
       "\u001b[32mattention, multi-head\\nattention and the parameter-free position representation and became the other person \u001b[0m\n",
       "\u001b[32minvolved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our \u001b[0m\n",
       "\u001b[32moriginal codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our \u001b[0m\n",
       "\u001b[32minitial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing\u001b[0m\n",
       "\u001b[32mvarious parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and \u001b[0m\n",
       "\u001b[32mmassively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google \u001b[0m\n",
       "\u001b[32mResearch.\\n31st Conference on Neural Information Processing Systems \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNIPS 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Long Beach, CA, \u001b[0m\n",
       "\u001b[32mUSA.\\narXiv:1706.03762v7  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  2 Aug 2023'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf_splitter=RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200)\n",
    "pdf_chunks=pdf_splitter.split_documents(pdf)\n",
    "print(pdf_chunks[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64cd4bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding=OllamaEmbeddings()\n",
    "pdf_emded=embedding.embed_documents(pdf_chunks)\n",
    "len(pdf_emded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba054ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1276629b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db=Chroma.from_documents(documents=pdf_chunks,embedding=embedding)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bb5ec78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b305e7bc-d53d-48e4-b3b0-22fd1bb0aaa1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16],</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">advantages over models such as [17, 18] and [9].\\n3 Model Architecture'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'b305e7bc-d53d-48e4-b3b0-22fd1bb0aaa1'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "        \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "        \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "        \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "        \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "        \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "        \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "        \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 \u001b[0m\n",
       "\u001b[32mBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\u001b[0m\n",
       "\u001b[32mByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use convolutional neural networks as basic building\\nblock, computing \u001b[0m\n",
       "\u001b[32mhidden representations in parallel for all input and output positions. In these models,\\nthe number of operations \u001b[0m\n",
       "\u001b[32mrequired to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, \u001b[0m\n",
       "\u001b[32mlinearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between \u001b[0m\n",
       "\u001b[32mdistant positions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost\u001b[0m\n",
       "\u001b[32mof reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with \u001b[0m\n",
       "\u001b[32mMulti-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an \u001b[0m\n",
       "\u001b[32mattention mechanism relating different positions\\nof a single sequence in order to compute a representation of the \u001b[0m\n",
       "\u001b[32msequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, \u001b[0m\n",
       "\u001b[32mabstractive summarization,\\ntextual entailment and learning task-independent sentence representations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, \u001b[0m\n",
       "\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned \u001b[0m\n",
       "\u001b[32mrecurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on \u001b[0m\n",
       "\u001b[32mself-attention to compute representations of its input and output without using sequence-\\naligned RNNs or \u001b[0m\n",
       "\u001b[32mconvolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its \u001b[0m\n",
       "\u001b[32madvantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n3 Model Architecture'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=db.similarity_search(\"what is Abstract ?\")\n",
    "print(result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d4521",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40700587",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=Chroma.from_documents(documents=pdf_chunks,embedding=embedding,persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ce2fb",
   "metadata": {},
   "source": [
    "### load chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d75227d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=Chroma(persist_directory=\"./chroma_db\",embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97860ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11419.7177734375</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m11419.7177734375\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result2=db2.similarity_search_with_score(\"what is Abstract ?\")\n",
    "print(result2[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f6e75",
   "metadata": {},
   "source": [
    "#### Retriever option \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6152f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db2.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c0f3c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f151c91b-0904-4b1f-8f09-7de194ba7e72'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../3.2-dataIngestion/attention.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-03T00:07:29+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16],</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">advantages over models such as [17, 18] and [9].\\n3 Model Architecture'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'f151c91b-0904-4b1f-8f09-7de194ba7e72'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "        \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "        \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "        \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "        \u001b[32m'source'\u001b[0m: \u001b[32m'../3.2-dataIngestion/attention.pdf'\u001b[0m,\n",
       "        \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "        \u001b[32m'moddate'\u001b[0m: \u001b[32m'2023-08-03T00:07:29+00:00'\u001b[0m,\n",
       "        \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "        \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "        \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 \u001b[0m\n",
       "\u001b[32mBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\u001b[0m\n",
       "\u001b[32mByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use convolutional neural networks as basic building\\nblock, computing \u001b[0m\n",
       "\u001b[32mhidden representations in parallel for all input and output positions. In these models,\\nthe number of operations \u001b[0m\n",
       "\u001b[32mrequired to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, \u001b[0m\n",
       "\u001b[32mlinearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between \u001b[0m\n",
       "\u001b[32mdistant positions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost\u001b[0m\n",
       "\u001b[32mof reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with \u001b[0m\n",
       "\u001b[32mMulti-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an \u001b[0m\n",
       "\u001b[32mattention mechanism relating different positions\\nof a single sequence in order to compute a representation of the \u001b[0m\n",
       "\u001b[32msequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, \u001b[0m\n",
       "\u001b[32mabstractive summarization,\\ntextual entailment and learning task-independent sentence representations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, \u001b[0m\n",
       "\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned \u001b[0m\n",
       "\u001b[32mrecurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on \u001b[0m\n",
       "\u001b[32mself-attention to compute representations of its input and output without using sequence-\\naligned RNNs or \u001b[0m\n",
       "\u001b[32mconvolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its \u001b[0m\n",
       "\u001b[32madvantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n3 Model Architecture'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever.invoke(\"what is Abstract ?\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3936a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
