{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a49d8a",
   "metadata": {},
   "source": [
    "## Getting Started with Langchain and openAI\n",
    "\n",
    "- set up with langchain , langsmith and langserve\n",
    "\n",
    "- use the most basic and common components of langchain : prompt templates  , models and output parsers.\n",
    "\n",
    "- build a simple application with langchain.\n",
    "- Trace your application with langsmith.\n",
    "- serve you application with langserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32967c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "load_dotenv(dotenv_path=\"notebook/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2ba20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Langsmith Tracking \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf3dd41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">openai.resources.chat.completions.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x124dd4d00</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x124dd4910</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">root_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.OpenAI object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x124dd46d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">root_async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.AsyncOpenAI object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x124dd7040</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-5'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x124dd4d00\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x124dd4910\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_client\u001b[0m\u001b[39m=<openai.OpenAI object at \u001b[0m\u001b[1;36m0x124dd46d0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_async_client\u001b[0m\u001b[39m=<openai.AsyncOpenAI object at \u001b[0m\u001b[1;36m0x124dd7040\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mmodel_name\u001b[0m=\u001b[32m'gpt-5'\u001b[0m,\n",
       "    \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm_openAI=ChatOpenAI(model=\"gpt-5\")\n",
    "print(llm_openAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a4b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get respomse from llm\n",
    "result=llm.invoke(\"What is agentic ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9db47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Agentic AI refers to AI systems that can pursue goals by deciding what to do next, taking actions, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">observing results, and iterating—rather than only responding to a single prompt. In practice, they combine a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning/planning core (often an LLM) with tools, memory, and feedback loops to operate semi‑autonomously.\\n\\nKey </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characteristics\\n- Goal-directed behavior: Given an objective, the system plans steps and selects actions.\\n- Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use: Calls APIs, runs code, queries databases, controls software or robots.\\n- Perception and feedback: Reads </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs, errors, or environment state and adapts.\\n- Memory/state: Stores context, results, and lessons to inform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">future steps.\\n- Self-reflection/critique: Evaluates its own plans and revises them.\\n- Adjustable autonomy: From </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistive (human-in-the-loop) to fully autonomous.\\n\\nHow it typically works (sense–plan–act loop)\\n1) Interpret </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">goal and constraints.\\n2) Decompose into tasks and plan.\\n3) Execute via tools or actions.\\n4) Observe outcomes and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">errors.\\n5) Update memory/plan and repeat until done or escalated.\\n\\nExamples and use cases\\n- Software agents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that file tickets, write code, run tests, and open pull requests.\\n- Research and analysis assistants that search, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">read documents, cite sources, and draft summaries.\\n- Business process automation (RPA 2.0): invoicing, data entry,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CRM updates.\\n- Customer support triage and resolution with system/tool access.\\n- DevOps/SRE runbooks, incident </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">response, and remediation.\\n- Trading/ops bots, scheduling assistants, and robotic control.\\n\\nBenefits\\n- Handles </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-step tasks end-to-end.\\n- Reduces manual effort and context switching.\\n- Can operate continuously and scale </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across workflows.\\n\\nRisks and challenges\\n- Reliability and hallucinations at scale; compounding errors across </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">steps.\\n- Safety/security: prompt injection, data exfiltration, tool misuse.\\n- Alignment with policies and legal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constraints.\\n- Cost and latency from long tool-use chains.\\n\\nCommon safeguards\\n- Clear objectives, constraints, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and allowed tools.\\n- Sandboxed execution, least-privilege access, and rate limits.\\n- Human-in-the-loop approvals </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for risky actions.\\n- Auditing, reproducible traces, and evals on real workflows.\\n\\nRelation to LLMs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">“ordinary” chatbots\\n- A standard chatbot mostly predicts text responses.\\n- An agentic system wraps the model with</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">planning, memory, and tool/actuation layers so it can decide and do, not just say.\\n\\nPopular building blocks\\n- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Orchestrators/agent frameworks, tool plugins/APIs, vector stores for memory, code interpreters, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluators/critics.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1009</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1021</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">448</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-5-2025-08-07'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-C6DP9tJTD0td2i35RHyWpZyu7PWUf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'service_tier'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run--92bdf1d5-053c-4918-bffd-7a5a839ba326-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1009</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1021</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">448</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Agentic AI refers to AI systems that can pursue goals by deciding what to do next, taking actions, \u001b[0m\n",
       "\u001b[32mobserving results, and iterating—rather than only responding to a single prompt. In practice, they combine a \u001b[0m\n",
       "\u001b[32mreasoning/planning core \u001b[0m\u001b[32m(\u001b[0m\u001b[32moften an LLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with tools, memory, and feedback loops to operate semi‑autonomously.\\n\\nKey \u001b[0m\n",
       "\u001b[32mcharacteristics\\n- Goal-directed behavior: Given an objective, the system plans steps and selects actions.\\n- Tool \u001b[0m\n",
       "\u001b[32muse: Calls APIs, runs code, queries databases, controls software or robots.\\n- Perception and feedback: Reads \u001b[0m\n",
       "\u001b[32moutputs, errors, or environment state and adapts.\\n- Memory/state: Stores context, results, and lessons to inform \u001b[0m\n",
       "\u001b[32mfuture steps.\\n- Self-reflection/critique: Evaluates its own plans and revises them.\\n- Adjustable autonomy: From \u001b[0m\n",
       "\u001b[32massistive \u001b[0m\u001b[32m(\u001b[0m\u001b[32mhuman-in-the-loop\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to fully autonomous.\\n\\nHow it typically works \u001b[0m\u001b[32m(\u001b[0m\u001b[32msense–plan–act loop\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Interpret \u001b[0m\n",
       "\u001b[32mgoal and constraints.\\n2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Decompose into tasks and plan.\\n3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Execute via tools or actions.\\n4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Observe outcomes and\u001b[0m\n",
       "\u001b[32merrors.\\n5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Update memory/plan and repeat until done or escalated.\\n\\nExamples and use cases\\n- Software agents \u001b[0m\n",
       "\u001b[32mthat file tickets, write code, run tests, and open pull requests.\\n- Research and analysis assistants that search, \u001b[0m\n",
       "\u001b[32mread documents, cite sources, and draft summaries.\\n- Business process automation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRPA 2.0\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: invoicing, data entry,\u001b[0m\n",
       "\u001b[32mCRM updates.\\n- Customer support triage and resolution with system/tool access.\\n- DevOps/SRE runbooks, incident \u001b[0m\n",
       "\u001b[32mresponse, and remediation.\\n- Trading/ops bots, scheduling assistants, and robotic control.\\n\\nBenefits\\n- Handles \u001b[0m\n",
       "\u001b[32mmulti-step tasks end-to-end.\\n- Reduces manual effort and context switching.\\n- Can operate continuously and scale \u001b[0m\n",
       "\u001b[32macross workflows.\\n\\nRisks and challenges\\n- Reliability and hallucinations at scale; compounding errors across \u001b[0m\n",
       "\u001b[32msteps.\\n- Safety/security: prompt injection, data exfiltration, tool misuse.\\n- Alignment with policies and legal \u001b[0m\n",
       "\u001b[32mconstraints.\\n- Cost and latency from long tool-use chains.\\n\\nCommon safeguards\\n- Clear objectives, constraints, \u001b[0m\n",
       "\u001b[32mand allowed tools.\\n- Sandboxed execution, least-privilege access, and rate limits.\\n- Human-in-the-loop approvals \u001b[0m\n",
       "\u001b[32mfor risky actions.\\n- Auditing, reproducible traces, and evals on real workflows.\\n\\nRelation to LLMs and \u001b[0m\n",
       "\u001b[32m“ordinary” chatbots\\n- A standard chatbot mostly predicts text responses.\\n- An agentic system wraps the model with\u001b[0m\n",
       "\u001b[32mplanning, memory, and tool/actuation layers so it can decide and do, not just say.\\n\\nPopular building blocks\\n- \u001b[0m\n",
       "\u001b[32mOrchestrators/agent frameworks, tool plugins/APIs, vector stores for memory, code interpreters, and \u001b[0m\n",
       "\u001b[32mevaluators/critics.'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m1009\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m1021\u001b[0m,\n",
       "            \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m448\u001b[0m,\n",
       "                \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-5-2025-08-07'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[32m'chatcmpl-C6DP9tJTD0td2i35RHyWpZyu7PWUf'\u001b[0m,\n",
       "        \u001b[32m'service_tier'\u001b[0m: \u001b[32m'default'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run--92bdf1d5-053c-4918-bffd-7a5a839ba326-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m1009\u001b[0m,\n",
       "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m1021\u001b[0m,\n",
       "        \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cache_read'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'reasoning'\u001b[0m: \u001b[1;36m448\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac202b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">groq.resources.chat.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x1249650c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;groq.resources.chat.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x1249e1cf0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai/gpt-oss-20b'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">groq_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mgroq.resources.chat.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x1249650c0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<groq.resources.chat.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x1249e1cf0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mmodel_name\u001b[0m=\u001b[32m'openai/gpt-oss-20b'\u001b[0m,\n",
       "    \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mgroq_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a83c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_qroq=llm.invoke(\"What is agentic ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc089bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">**Agentic AI** – *the “agency” in artificial intelligence*\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. What is it?\n",
       "\n",
       "| Feature | What it means | Typical AI that has it |\n",
       "|---------|----------------|------------------------|\n",
       "| **Agency** | The system has a *sense of “doing”*—it can set goals, plan to achieve them, and execute actions. | \n",
       "Autonomous cars, robo‑traders, personal assistant bots |\n",
       "| **Goal‑directedness** | It pursues objectives that are *explicit or learned*, not just react to input. | \n",
       "Reinforcement‑learning agents, BDI <span style=\"font-weight: bold\">(</span>Belief‑Desire‑Intention<span style=\"font-weight: bold\">)</span> agents |\n",
       "| **Autonomy** | It can operate without continuous human oversight, though humans may set high‑level constraints. |\n",
       "Self‑driving drones, autonomous manufacturing robots |\n",
       "| **Adaptivity** | It learns from experience to improve its behavior over time. | Adaptive recommendation systems, \n",
       "dynamic resource‑allocation bots |\n",
       "| **Self‑modeling <span style=\"font-weight: bold\">(</span>sometimes<span style=\"font-weight: bold\">)</span>** | It has an internal representation of its own state or capabilities, which it uses\n",
       "for planning. | Advanced planning agents in robotics, some AI‑driven game‑AI |\n",
       "\n",
       "In short, *agentic AI* is an AI that behaves like a “little agent” in the world: it has *intentions* <span style=\"font-weight: bold\">(</span>goals<span style=\"font-weight: bold\">)</span>, \n",
       "*means* <span style=\"font-weight: bold\">(</span>plans<span style=\"font-weight: bold\">)</span>, and *action* <span style=\"font-weight: bold\">(</span>behaviour<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Why the term “agentic”?\n",
       "\n",
       "The term comes from **agentic theory** in psychology <span style=\"font-weight: bold\">(</span>the idea that people act with intention<span style=\"font-weight: bold\">)</span> and is borrowed to \n",
       "describe machines that *act intentionally*.  It contrasts with:\n",
       "\n",
       "| Agentic | Reactive |\n",
       "|---------|----------|\n",
       "| Goal‑driven, plans, can change goals | Follows rules or triggers, no internal goal representation |\n",
       "| Example: a robot that decides to recharge itself | Example: a thermostat that turns on when temperature drops |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Where do we see it today?\n",
       "\n",
       "| Domain | Example of agentic AI |\n",
       "|--------|-----------------------|\n",
       "| **Transportation** | Self‑driving cars, autonomous cargo ships |\n",
       "| **Finance** | Algorithmic trading bots that set portfolio goals |\n",
       "| **Healthcare** | AI‑driven treatment planners that choose therapy sequences |\n",
       "| **Robotics** | Warehouse robots that plan paths and re‑plan on the fly |\n",
       "| **Customer Service** | Conversational agents that set service‑level goals <span style=\"font-weight: bold\">(</span>e.g., “resolve in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> min”<span style=\"font-weight: bold\">)</span> |\n",
       "| **Gaming &amp; Simulation** | NPCs that pursue quests or adapt to player strategy |\n",
       "| **Energy** | Smart grids that autonomously balance supply and demand |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Core research themes\n",
       "\n",
       "| Theme | Why it matters for agentic AI |\n",
       "|-------|------------------------------|\n",
       "| **Planning &amp; Decision‑Making** | How the agent chooses actions in uncertain environments <span style=\"font-weight: bold\">(</span>e.g., MDPs, POMDPs, \n",
       "hierarchical planning<span style=\"font-weight: bold\">)</span>. |\n",
       "| **Learning &amp; Adaptation** | Reinforcement learning, meta‑learning, continual learning. |\n",
       "| **Goal Alignment &amp; Safety** | Ensuring the agent’s goals match human values <span style=\"font-weight: bold\">(</span>alignment problem<span style=\"font-weight: bold\">)</span>. |\n",
       "| **Interpretability &amp; Transparency** | Making the agent’s intentions understandable to humans. |\n",
       "| **Control &amp; Governance** | Designing interfaces, constraints, and oversight mechanisms. |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Benefits\n",
       "\n",
       "| Benefit | Example |\n",
       "|---------|---------|\n",
       "| **Efficiency** | Robots that self‑schedule maintenance, saving downtime. |\n",
       "| **Personalization** | AI that adapts recommendations to a user’s evolving preferences. |\n",
       "| **Scalability** | Autonomous systems that can operate in many locations without human supervision. |\n",
       "| **Safety** | Self‑driving cars that can anticipate and avoid hazards. |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. Risks &amp; Ethical Concerns\n",
       "\n",
       "| Concern | What it looks like |\n",
       "|---------|--------------------|\n",
       "| **Misaligned Goals** | A trading bot that maximizes profit at the expense of market stability. |\n",
       "| **Unpredictability** | An autonomous vehicle that takes an unexpected detour. |\n",
       "| **Accountability** | Who is liable when an agent causes harm? |\n",
       "| **Bias &amp; Fairness** | An agent that learns biased patterns from data <span style=\"font-weight: bold\">(</span>e.g., hiring bots<span style=\"font-weight: bold\">)</span>. |\n",
       "| **Privacy** | Agents that gather personal data to optimize goals. |\n",
       "| **Control Problem** | Theoretical risk that highly capable agents could act against human wishes. |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. Key References <span style=\"font-weight: bold\">(</span>for deeper reading<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "| Topic | Reference |\n",
       "|-------|-----------|\n",
       "| Agentic AI definition &amp; taxonomy | Wooldridge, M. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2009</span><span style=\"font-weight: bold\">)</span>. *An Introduction to Multi‑Agent Systems*. |\n",
       "| BDI agents | Rao, A. S., &amp; Georgeff, M. P. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1995</span><span style=\"font-weight: bold\">)</span>. *Reasoning about Goals and Intention*. |\n",
       "| Reinforcement learning for agents | Sutton, R. S., &amp; Barto, A. G. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"font-weight: bold\">)</span>. *Reinforcement Learning: An \n",
       "Introduction*. |\n",
       "| Alignment &amp; safety | Amodei, D., et al. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"font-weight: bold\">)</span>. *Concrete Problems in AI Safety*. |\n",
       "| Governance &amp; ethics | Bostrom, N., &amp; Yudkowsky, E. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"font-weight: bold\">)</span>. *The Ethics of Artificial Intelligence*. |\n",
       "\n",
       "---\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. Bottom line\n",
       "\n",
       "**Agentic AI** is the branch of AI that builds systems capable of *acting with purpose*.  These systems can plan, \n",
       "learn, and adapt to achieve goals, often with minimal human intervention.  While they promise great efficiencies \n",
       "and new capabilities, they also raise important questions about alignment, safety, and accountability—issues that \n",
       "researchers, regulators, and society are actively grappling with today.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "**Agentic AI** – *the “agency” in artificial intelligence*\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m1\u001b[0m. What is it?\n",
       "\n",
       "| Feature | What it means | Typical AI that has it |\n",
       "|---------|----------------|------------------------|\n",
       "| **Agency** | The system has a *sense of “doing”*—it can set goals, plan to achieve them, and execute actions. | \n",
       "Autonomous cars, robo‑traders, personal assistant bots |\n",
       "| **Goal‑directedness** | It pursues objectives that are *explicit or learned*, not just react to input. | \n",
       "Reinforcement‑learning agents, BDI \u001b[1m(\u001b[0mBelief‑Desire‑Intention\u001b[1m)\u001b[0m agents |\n",
       "| **Autonomy** | It can operate without continuous human oversight, though humans may set high‑level constraints. |\n",
       "Self‑driving drones, autonomous manufacturing robots |\n",
       "| **Adaptivity** | It learns from experience to improve its behavior over time. | Adaptive recommendation systems, \n",
       "dynamic resource‑allocation bots |\n",
       "| **Self‑modeling \u001b[1m(\u001b[0msometimes\u001b[1m)\u001b[0m** | It has an internal representation of its own state or capabilities, which it uses\n",
       "for planning. | Advanced planning agents in robotics, some AI‑driven game‑AI |\n",
       "\n",
       "In short, *agentic AI* is an AI that behaves like a “little agent” in the world: it has *intentions* \u001b[1m(\u001b[0mgoals\u001b[1m)\u001b[0m, \n",
       "*means* \u001b[1m(\u001b[0mplans\u001b[1m)\u001b[0m, and *action* \u001b[1m(\u001b[0mbehaviour\u001b[1m)\u001b[0m.\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m2\u001b[0m. Why the term “agentic”?\n",
       "\n",
       "The term comes from **agentic theory** in psychology \u001b[1m(\u001b[0mthe idea that people act with intention\u001b[1m)\u001b[0m and is borrowed to \n",
       "describe machines that *act intentionally*.  It contrasts with:\n",
       "\n",
       "| Agentic | Reactive |\n",
       "|---------|----------|\n",
       "| Goal‑driven, plans, can change goals | Follows rules or triggers, no internal goal representation |\n",
       "| Example: a robot that decides to recharge itself | Example: a thermostat that turns on when temperature drops |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m3\u001b[0m. Where do we see it today?\n",
       "\n",
       "| Domain | Example of agentic AI |\n",
       "|--------|-----------------------|\n",
       "| **Transportation** | Self‑driving cars, autonomous cargo ships |\n",
       "| **Finance** | Algorithmic trading bots that set portfolio goals |\n",
       "| **Healthcare** | AI‑driven treatment planners that choose therapy sequences |\n",
       "| **Robotics** | Warehouse robots that plan paths and re‑plan on the fly |\n",
       "| **Customer Service** | Conversational agents that set service‑level goals \u001b[1m(\u001b[0me.g., “resolve in \u001b[1;36m5\u001b[0m min”\u001b[1m)\u001b[0m |\n",
       "| **Gaming & Simulation** | NPCs that pursue quests or adapt to player strategy |\n",
       "| **Energy** | Smart grids that autonomously balance supply and demand |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m4\u001b[0m. Core research themes\n",
       "\n",
       "| Theme | Why it matters for agentic AI |\n",
       "|-------|------------------------------|\n",
       "| **Planning & Decision‑Making** | How the agent chooses actions in uncertain environments \u001b[1m(\u001b[0me.g., MDPs, POMDPs, \n",
       "hierarchical planning\u001b[1m)\u001b[0m. |\n",
       "| **Learning & Adaptation** | Reinforcement learning, meta‑learning, continual learning. |\n",
       "| **Goal Alignment & Safety** | Ensuring the agent’s goals match human values \u001b[1m(\u001b[0malignment problem\u001b[1m)\u001b[0m. |\n",
       "| **Interpretability & Transparency** | Making the agent’s intentions understandable to humans. |\n",
       "| **Control & Governance** | Designing interfaces, constraints, and oversight mechanisms. |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m5\u001b[0m. Benefits\n",
       "\n",
       "| Benefit | Example |\n",
       "|---------|---------|\n",
       "| **Efficiency** | Robots that self‑schedule maintenance, saving downtime. |\n",
       "| **Personalization** | AI that adapts recommendations to a user’s evolving preferences. |\n",
       "| **Scalability** | Autonomous systems that can operate in many locations without human supervision. |\n",
       "| **Safety** | Self‑driving cars that can anticipate and avoid hazards. |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m6\u001b[0m. Risks & Ethical Concerns\n",
       "\n",
       "| Concern | What it looks like |\n",
       "|---------|--------------------|\n",
       "| **Misaligned Goals** | A trading bot that maximizes profit at the expense of market stability. |\n",
       "| **Unpredictability** | An autonomous vehicle that takes an unexpected detour. |\n",
       "| **Accountability** | Who is liable when an agent causes harm? |\n",
       "| **Bias & Fairness** | An agent that learns biased patterns from data \u001b[1m(\u001b[0me.g., hiring bots\u001b[1m)\u001b[0m. |\n",
       "| **Privacy** | Agents that gather personal data to optimize goals. |\n",
       "| **Control Problem** | Theoretical risk that highly capable agents could act against human wishes. |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m7\u001b[0m. Key References \u001b[1m(\u001b[0mfor deeper reading\u001b[1m)\u001b[0m\n",
       "\n",
       "| Topic | Reference |\n",
       "|-------|-----------|\n",
       "| Agentic AI definition & taxonomy | Wooldridge, M. \u001b[1m(\u001b[0m\u001b[1;36m2009\u001b[0m\u001b[1m)\u001b[0m. *An Introduction to Multi‑Agent Systems*. |\n",
       "| BDI agents | Rao, A. S., & Georgeff, M. P. \u001b[1m(\u001b[0m\u001b[1;36m1995\u001b[0m\u001b[1m)\u001b[0m. *Reasoning about Goals and Intention*. |\n",
       "| Reinforcement learning for agents | Sutton, R. S., & Barto, A. G. \u001b[1m(\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1m)\u001b[0m. *Reinforcement Learning: An \n",
       "Introduction*. |\n",
       "| Alignment & safety | Amodei, D., et al. \u001b[1m(\u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1m)\u001b[0m. *Concrete Problems in AI Safety*. |\n",
       "| Governance & ethics | Bostrom, N., & Yudkowsky, E. \u001b[1m(\u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1m)\u001b[0m. *The Ethics of Artificial Intelligence*. |\n",
       "\n",
       "---\n",
       "\n",
       "### \u001b[1;36m8\u001b[0m. Bottom line\n",
       "\n",
       "**Agentic AI** is the branch of AI that builds systems capable of *acting with purpose*.  These systems can plan, \n",
       "learn, and adapt to achieve goals, often with minimal human intervention.  While they promise great efficiencies \n",
       "and new capabilities, they also raise important questions about alignment, safety, and accountability—issues that \n",
       "researchers, regulators, and society are actively grappling with today.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(result_qroq.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe58b3",
   "metadata": {},
   "source": [
    "#### Chat Prompt template \n",
    "\n",
    "prompt template is a set of prompts given to llm before human input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bd6a722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'you are an expert AI Engineer.Provide me answers based on the questions.'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{input}'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'you are an expert AI Engineer.Provide me answers based on the questions.'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are an expert AI Engineer.Provide me answers based on the questions.\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54af6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41fddeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">## What is Langsmith?\n",
       "\n",
       "**Langsmith** is a purpose‑built observability platform for large‑language‑model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> applications.  \n",
       "It lets you instrument, trace, debug, and monitor every call that your code makes to an LLM, a vector store, or any\n",
       "other downstream service.  \n",
       "\n",
       "Think of it as the “Datadog for LLM pipelines.”  It gives you a single pane of glass that shows:\n",
       "\n",
       "| Feature | What it does | Why it matters |\n",
       "|---------|--------------|----------------|\n",
       "| **Tracing** | Records each prompt, response, and the chain of calls that led to it | Lets you replay a run, see \n",
       "where a failure happened, and understand the flow of data |\n",
       "| **Metrics** | Tracks latency, token usage, cost, and custom metrics | Helps you optimize for speed, cost, and SLA\n",
       "|\n",
       "| **Debug UI** | Interactive view of prompts, embeddings, and tool calls | Spot bugs, typos, or logic errors in \n",
       "your chain |\n",
       "| **Search &amp; Filter** | Query traces by user, model, timestamp, or error | Quickly find the problematic run |\n",
       "| **A/B Testing** | Compare two different prompts or models side‑by‑side | Data‑driven model selection |\n",
       "| **Compliance &amp; Auditing** | Keep a tamper‑proof record of every request | Meets data‑protection regulations |\n",
       "| **SDK &amp; Decorators** | Simple `@instrument` or `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.track</span><span style=\"font-weight: bold\">()</span>` calls | Zero‑boilerplate integration with \n",
       "LangChain, LlamaIndex, or raw OpenAI calls |\n",
       "\n",
       "---\n",
       "\n",
       "## How does it fit into the LLM stack?\n",
       "\n",
       "| Stack | Typical Use | Langsmith Role |\n",
       "|-------|-------------|----------------|\n",
       "| **LangChain** | Building chains, agents, prompts | Instrument every `LLM`, `Tool`, or `Chain` call |\n",
       "| **LlamaIndex** | Data ingestion &amp; query | Log vector store queries, embeddings, and LLM responses |\n",
       "| **FastAPI <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Flask** | API endpoints that call LLMs | Wrap route handlers to automatically trace requests |\n",
       "| **Streamlit <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Gradio** | Demo apps | Log user interactions and LLM outputs |\n",
       "| **Custom code** | Any OpenAI <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Anthropic <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Cohere call | Use the low‑level `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.track</span><span style=\"font-weight: bold\">()</span>` or the SDK wrapper\n",
       "|\n",
       "\n",
       "Because Langsmith is agnostic to the framework, you can drop it into an existing codebase with a few lines of code.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick‑start\n",
       "\n",
       "```bash\n",
       "# <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>️⃣ Install the SDK\n",
       "pip install langsmith\n",
       "\n",
       "# <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>️⃣ Get an API key from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://smith.langchain.com</span>\n",
       "export <span style=\"color: #808000; text-decoration-color: #808000\">LANGSMITH_API_KEY</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"sk-xxxxxx\"</span>\n",
       "\n",
       "# <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>️⃣ Instrument your code\n",
       "from langsmith import instrument\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "\n",
       "# Wrap the LLM call\n",
       "@instrument\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat</span><span style=\"font-weight: bold\">(</span>prompt: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"gpt-4o-mini\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Tell me a short joke.\"</span><span style=\"font-weight: bold\">))</span>\n",
       "```\n",
       "\n",
       "Once you run the snippet, a trace will appear in the Langsmith web UI:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Prompt &amp; metadata** – see exactly what you sent.  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Response** – the full LLM output.  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Timing** – milliseconds spent, tokens generated, cost.  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Tool calls** – if you used an agent, all tool invocations are logged.\n",
       "\n",
       "You can also add custom tags:\n",
       "\n",
       "```python\n",
       "@<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">instrument</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tags</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"support\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"customer_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1234</span><span style=\"font-weight: bold\">})</span>\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">support_chat</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "## Core Concepts\n",
       "\n",
       "| Concept | Description |\n",
       "|---------|-------------|\n",
       "| **Trace** | A single LLM call <span style=\"font-weight: bold\">(</span>prompt → response<span style=\"font-weight: bold\">)</span>.  Includes metadata, timing, and cost. |\n",
       "| **Run** | A collection of traces that belong together <span style=\"font-weight: bold\">(</span>e.g., a single HTTP request or a user session<span style=\"font-weight: bold\">)</span>. |\n",
       "| **Node** | A component in a chain <span style=\"font-weight: bold\">(</span>LLM, PromptTemplate, Tool, etc.<span style=\"font-weight: bold\">)</span>.  Nodes are shown in the trace graph. |\n",
       "| **Metric** | Quantitative data you define <span style=\"font-weight: bold\">(</span>e.g., “average tokens per prompt”<span style=\"font-weight: bold\">)</span>.  Langsmith aggregates and \n",
       "visualises them. |\n",
       "\n",
       "---\n",
       "\n",
       "## Typical Use Cases\n",
       "\n",
       "| Scenario | How Langsmith Helps |\n",
       "|----------|---------------------|\n",
       "| **Debugging a broken chain** | Pinpoint the exact node that produced an error or an unexpected output. |\n",
       "| **Optimising cost** | Identify which prompts use the most tokens and which models are expensive. |\n",
       "| **Performance tuning** | See latency bottlenecks in a chain and decide whether to cache embeddings or pre‑compute\n",
       "prompts. |\n",
       "| **A/B testing** | Compare two prompt templates or two model versions side‑by‑side with real user data. |\n",
       "| **Compliance** | Keep a tamper‑proof audit trail of every request for regulatory or security reviews. |\n",
       "| **Developer onboarding** | New engineers can view live traces to understand how the system works. |\n",
       "\n",
       "---\n",
       "\n",
       "## Integration Highlights\n",
       "\n",
       "| Library | How to integrate |\n",
       "|---------|------------------|\n",
       "| **LangChain** | `from langsmith import LangChainTracer`  <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">br</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; `tracer = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LangChainTracer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">`  &lt;br&gt; </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">`</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tracer.add_chain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">chain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">` |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| **OpenAI SDK** | Use the `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.track</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` wrapper or the `openai` client’s `request` hook. |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| **FastAPI** | `from langsmith import FastAPITracer`  &lt;br&gt; `app = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FastAPITracer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">app</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">` |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| **Streamlit** | `from langsmith import StreamlitTracer`  &lt;br&gt; `st = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StreamlitTracer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| **Custom** | Wrap any async or sync function with `@instrument` or call `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.track</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` manually. |</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">---</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Pricing &amp; Availability</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- **Open‑Source Core** – The SDK and tracing logic are free and open‑source </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">MIT license</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- **Hosted SaaS** – Langsmith offers a managed service with a UI, dashboards, and enterprise features </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">audit logs, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">SSO, etc.</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- **Self‑Hosted** – You can run the open‑source backend on your own infrastructure </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">Docker Compose, Kubernetes, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">etc.</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&gt;</span> **Tip:** Start with the free tier. It gives you <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> traces/month and a basic UI, which is enough for most \n",
       "prototypes.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick FAQ\n",
       "\n",
       "| Question | Answer |\n",
       "|----------|--------|\n",
       "| *Can I use Langsmith without LangChain?* | Yes – just wrap your `openai.Completion.create` or any HTTP call with \n",
       "`<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.track</span><span style=\"font-weight: bold\">()</span>` or the SDK’s decorator. |\n",
       "| *Does it store the prompt text?* | By default it does. You can mask or redact sensitive data via the UI or the \n",
       "SDK’s `<span style=\"color: #808000; text-decoration-color: #808000\">mask_prompt</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>`. |\n",
       "| *Is it GDPR‑compliant?* | The hosted service offers data residency options and audit logs. The SDK lets you \n",
       "delete traces programmatically. |\n",
       "| *How many traces can I keep?* | Unlimited in the paid plan. The free tier keeps up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> traces/month. |\n",
       "| *Can I export traces?* | Yes – via the UI or the REST API <span style=\"font-weight: bold\">(</span>`GET <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">traces</span>`<span style=\"font-weight: bold\">)</span>. |\n",
       "| *What about latency overhead?* | The SDK is lightweight <span style=\"font-weight: bold\">(</span>~<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>–<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> ms per call<span style=\"font-weight: bold\">)</span>. For high‑frequency workloads, run the\n",
       "SDK in a separate thread or use the async API. |\n",
       "\n",
       "---\n",
       "\n",
       "## Getting Started Resources\n",
       "\n",
       "| Resource | Link |\n",
       "|----------|------|\n",
       "| **Documentation** | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.smith.langchain.com</span> |\n",
       "| **GitHub Repo** | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/langchain-ai/langsmith</span> |\n",
       "| **SDK Installation** | `pip install langsmith` |\n",
       "| **Example Notebooks** | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/langchain-ai/langsmith/tree/main/examples</span> |\n",
       "| **Community Slack** | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://langchain.slack.com</span> <span style=\"font-weight: bold\">(</span>invite link in docs<span style=\"font-weight: bold\">)</span> |\n",
       "\n",
       "---\n",
       "\n",
       "### Bottom line\n",
       "\n",
       "Langsmith gives you **end‑to‑end observability** for your LLM pipelines.  \n",
       "Whether you’re debugging a single prompt, monitoring a production agent, or comparing models for cost, Langsmith \n",
       "turns raw LLM calls into searchable, visualised data that you can act on.  \n",
       "\n",
       "Happy building—and tracing! 🚀\n",
       "</pre>\n"
      ],
      "text/plain": [
       "## What is Langsmith?\n",
       "\n",
       "**Langsmith** is a purpose‑built observability platform for large‑language‑model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m applications.  \n",
       "It lets you instrument, trace, debug, and monitor every call that your code makes to an LLM, a vector store, or any\n",
       "other downstream service.  \n",
       "\n",
       "Think of it as the “Datadog for LLM pipelines.”  It gives you a single pane of glass that shows:\n",
       "\n",
       "| Feature | What it does | Why it matters |\n",
       "|---------|--------------|----------------|\n",
       "| **Tracing** | Records each prompt, response, and the chain of calls that led to it | Lets you replay a run, see \n",
       "where a failure happened, and understand the flow of data |\n",
       "| **Metrics** | Tracks latency, token usage, cost, and custom metrics | Helps you optimize for speed, cost, and SLA\n",
       "|\n",
       "| **Debug UI** | Interactive view of prompts, embeddings, and tool calls | Spot bugs, typos, or logic errors in \n",
       "your chain |\n",
       "| **Search & Filter** | Query traces by user, model, timestamp, or error | Quickly find the problematic run |\n",
       "| **A/B Testing** | Compare two different prompts or models side‑by‑side | Data‑driven model selection |\n",
       "| **Compliance & Auditing** | Keep a tamper‑proof record of every request | Meets data‑protection regulations |\n",
       "| **SDK & Decorators** | Simple `@instrument` or `\u001b[1;35mlangsmith.track\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` calls | Zero‑boilerplate integration with \n",
       "LangChain, LlamaIndex, or raw OpenAI calls |\n",
       "\n",
       "---\n",
       "\n",
       "## How does it fit into the LLM stack?\n",
       "\n",
       "| Stack | Typical Use | Langsmith Role |\n",
       "|-------|-------------|----------------|\n",
       "| **LangChain** | Building chains, agents, prompts | Instrument every `LLM`, `Tool`, or `Chain` call |\n",
       "| **LlamaIndex** | Data ingestion & query | Log vector store queries, embeddings, and LLM responses |\n",
       "| **FastAPI \u001b[35m/\u001b[0m Flask** | API endpoints that call LLMs | Wrap route handlers to automatically trace requests |\n",
       "| **Streamlit \u001b[35m/\u001b[0m Gradio** | Demo apps | Log user interactions and LLM outputs |\n",
       "| **Custom code** | Any OpenAI \u001b[35m/\u001b[0m Anthropic \u001b[35m/\u001b[0m Cohere call | Use the low‑level `\u001b[1;35mlangsmith.track\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` or the SDK wrapper\n",
       "|\n",
       "\n",
       "Because Langsmith is agnostic to the framework, you can drop it into an existing codebase with a few lines of code.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick‑start\n",
       "\n",
       "```bash\n",
       "# \u001b[1;36m1\u001b[0m️⃣ Install the SDK\n",
       "pip install langsmith\n",
       "\n",
       "# \u001b[1;36m2\u001b[0m️⃣ Get an API key from \u001b[4;94mhttps://smith.langchain.com\u001b[0m\n",
       "export \u001b[33mLANGSMITH_API_KEY\u001b[0m=\u001b[32m\"sk\u001b[0m\u001b[32m-xxxxxx\"\u001b[0m\n",
       "\n",
       "# \u001b[1;36m3\u001b[0m️⃣ Instrument your code\n",
       "from langsmith import instrument\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "\n",
       "# Wrap the LLM call\n",
       "@instrument\n",
       "def \u001b[1;35mchat\u001b[0m\u001b[1m(\u001b[0mprompt: str\u001b[1m)\u001b[0m -> str:\n",
       "    llm = \u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m\"gpt\u001b[0m\u001b[32m-4o-mini\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return \u001b[1;35mllm\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mchat\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Tell me a short joke.\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Once you run the snippet, a trace will appear in the Langsmith web UI:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Prompt & metadata** – see exactly what you sent.  \n",
       "\u001b[1;36m2\u001b[0m. **Response** – the full LLM output.  \n",
       "\u001b[1;36m3\u001b[0m. **Timing** – milliseconds spent, tokens generated, cost.  \n",
       "\u001b[1;36m4\u001b[0m. **Tool calls** – if you used an agent, all tool invocations are logged.\n",
       "\n",
       "You can also add custom tags:\n",
       "\n",
       "```python\n",
       "@\u001b[1;35minstrument\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtags\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m\"role\"\u001b[0m: \u001b[32m\"support\"\u001b[0m, \u001b[32m\"customer_id\"\u001b[0m: \u001b[1;36m1234\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "def \u001b[1;35msupport_chat\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m:\n",
       "    \u001b[33m...\u001b[0m\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "## Core Concepts\n",
       "\n",
       "| Concept | Description |\n",
       "|---------|-------------|\n",
       "| **Trace** | A single LLM call \u001b[1m(\u001b[0mprompt → response\u001b[1m)\u001b[0m.  Includes metadata, timing, and cost. |\n",
       "| **Run** | A collection of traces that belong together \u001b[1m(\u001b[0me.g., a single HTTP request or a user session\u001b[1m)\u001b[0m. |\n",
       "| **Node** | A component in a chain \u001b[1m(\u001b[0mLLM, PromptTemplate, Tool, etc.\u001b[1m)\u001b[0m.  Nodes are shown in the trace graph. |\n",
       "| **Metric** | Quantitative data you define \u001b[1m(\u001b[0me.g., “average tokens per prompt”\u001b[1m)\u001b[0m.  Langsmith aggregates and \n",
       "visualises them. |\n",
       "\n",
       "---\n",
       "\n",
       "## Typical Use Cases\n",
       "\n",
       "| Scenario | How Langsmith Helps |\n",
       "|----------|---------------------|\n",
       "| **Debugging a broken chain** | Pinpoint the exact node that produced an error or an unexpected output. |\n",
       "| **Optimising cost** | Identify which prompts use the most tokens and which models are expensive. |\n",
       "| **Performance tuning** | See latency bottlenecks in a chain and decide whether to cache embeddings or pre‑compute\n",
       "prompts. |\n",
       "| **A/B testing** | Compare two prompt templates or two model versions side‑by‑side with real user data. |\n",
       "| **Compliance** | Keep a tamper‑proof audit trail of every request for regulatory or security reviews. |\n",
       "| **Developer onboarding** | New engineers can view live traces to understand how the system works. |\n",
       "\n",
       "---\n",
       "\n",
       "## Integration Highlights\n",
       "\n",
       "| Library | How to integrate |\n",
       "|---------|------------------|\n",
       "| **LangChain** | `from langsmith import LangChainTracer`  \u001b[1m<\u001b[0m\u001b[1;95mbr\u001b[0m\u001b[39m> `tracer = \u001b[0m\u001b[1;35mLangChainTracer\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m`  <br> \u001b[0m\n",
       "\u001b[39m`\u001b[0m\u001b[1;35mtracer.add_chain\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mchain\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` |\u001b[0m\n",
       "\u001b[39m| **OpenAI SDK** | Use the `\u001b[0m\u001b[1;35mlangsmith.track\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` wrapper or the `openai` client’s `request` hook. |\u001b[0m\n",
       "\u001b[39m| **FastAPI** | `from langsmith import FastAPITracer`  <br> `app = \u001b[0m\u001b[1;35mFastAPITracer\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mapp\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` |\u001b[0m\n",
       "\u001b[39m| **Streamlit** | `from langsmith import StreamlitTracer`  <br> `st = \u001b[0m\u001b[1;35mStreamlitTracer\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` |\u001b[0m\n",
       "\u001b[39m| **Custom** | Wrap any async or sync function with `@instrument` or call `\u001b[0m\u001b[1;35mlangsmith.track\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` manually. |\u001b[0m\n",
       "\n",
       "\u001b[39m---\u001b[0m\n",
       "\n",
       "\u001b[39m## Pricing & Availability\u001b[0m\n",
       "\n",
       "\u001b[39m- **Open‑Source Core** – The SDK and tracing logic are free and open‑source \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mMIT license\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.  \u001b[0m\n",
       "\u001b[39m- **Hosted SaaS** – Langsmith offers a managed service with a UI, dashboards, and enterprise features \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39maudit logs, \u001b[0m\n",
       "\u001b[39mSSO, etc.\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.  \u001b[0m\n",
       "\u001b[39m- **Self‑Hosted** – You can run the open‑source backend on your own infrastructure \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mDocker Compose, Kubernetes, \u001b[0m\n",
       "\u001b[39metc.\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[1m>\u001b[0m **Tip:** Start with the free tier. It gives you \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m traces/month and a basic UI, which is enough for most \n",
       "prototypes.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick FAQ\n",
       "\n",
       "| Question | Answer |\n",
       "|----------|--------|\n",
       "| *Can I use Langsmith without LangChain?* | Yes – just wrap your `openai.Completion.create` or any HTTP call with \n",
       "`\u001b[1;35mlangsmith.track\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` or the SDK’s decorator. |\n",
       "| *Does it store the prompt text?* | By default it does. You can mask or redact sensitive data via the UI or the \n",
       "SDK’s `\u001b[33mmask_prompt\u001b[0m=\u001b[3;92mTrue\u001b[0m`. |\n",
       "| *Is it GDPR‑compliant?* | The hosted service offers data residency options and audit logs. The SDK lets you \n",
       "delete traces programmatically. |\n",
       "| *How many traces can I keep?* | Unlimited in the paid plan. The free tier keeps up to \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m traces/month. |\n",
       "| *Can I export traces?* | Yes – via the UI or the REST API \u001b[1m(\u001b[0m`GET \u001b[35m/\u001b[0m\u001b[95mtraces\u001b[0m`\u001b[1m)\u001b[0m. |\n",
       "| *What about latency overhead?* | The SDK is lightweight \u001b[1m(\u001b[0m~\u001b[1;36m1\u001b[0m–\u001b[1;36m2\u001b[0m ms per call\u001b[1m)\u001b[0m. For high‑frequency workloads, run the\n",
       "SDK in a separate thread or use the async API. |\n",
       "\n",
       "---\n",
       "\n",
       "## Getting Started Resources\n",
       "\n",
       "| Resource | Link |\n",
       "|----------|------|\n",
       "| **Documentation** | \u001b[4;94mhttps://docs.smith.langchain.com\u001b[0m |\n",
       "| **GitHub Repo** | \u001b[4;94mhttps://github.com/langchain-ai/langsmith\u001b[0m |\n",
       "| **SDK Installation** | `pip install langsmith` |\n",
       "| **Example Notebooks** | \u001b[4;94mhttps://github.com/langchain-ai/langsmith/tree/main/examples\u001b[0m |\n",
       "| **Community Slack** | \u001b[4;94mhttps://langchain.slack.com\u001b[0m \u001b[1m(\u001b[0minvite link in docs\u001b[1m)\u001b[0m |\n",
       "\n",
       "---\n",
       "\n",
       "### Bottom line\n",
       "\n",
       "Langsmith gives you **end‑to‑end observability** for your LLM pipelines.  \n",
       "Whether you’re debugging a single prompt, monitoring a production agent, or comparing models for cost, Langsmith \n",
       "turns raw LLM calls into searchable, visualised data that you can act on.  \n",
       "\n",
       "Happy building—and tracing! 🚀\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01f1b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_openAi=prompt|llm_openAI\n",
    "response_openAI=chain_openAi.invoke({\"input\":\"Can you tell me about Langsmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e4e97b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LangSmith <span style=\"font-weight: bold\">(</span>by LangChain<span style=\"font-weight: bold\">)</span> is an LLM app development platform for tracing, testing, evaluating, and monitoring AI \n",
       "chains/agents in both development and production. Think of it as observability + experiment tracking + evaluation \n",
       "for LLM systems, model/framework agnostic, with SDKs for Python and JavaScript.\n",
       "\n",
       "What it does\n",
       "- Tracing and observability: Capture detailed, hierarchical traces of runs <span style=\"font-weight: bold\">(</span>chains, tools, model calls, retrievers<span style=\"font-weight: bold\">)</span>\n",
       "with inputs/outputs, tokens, latency, errors, and metadata. Useful for debugging complex LangChain/LangGraph \n",
       "pipelines and any custom code.\n",
       "- Evaluation and testing: Run offline or continuous evaluations with heuristic checks <span style=\"font-weight: bold\">(</span>exact match, regex<span style=\"font-weight: bold\">)</span>, \n",
       "semantic metrics <span style=\"font-weight: bold\">(</span>similarity, correctness<span style=\"font-weight: bold\">)</span>, and LLM-judge–based graders <span style=\"font-weight: bold\">(</span>e.g., faithfulness for RAG<span style=\"font-weight: bold\">)</span>. Supports \n",
       "dataset-based regression tests and pairwise comparisons across model/prompt variants.\n",
       "- Prompt and experiment management: Version prompts, compare variants, and record experiments with automatic \n",
       "lineage, tags, and metrics.\n",
       "- Datasets and annotation: Create datasets of inputs/expected outputs, collect human feedback/labels, and use them \n",
       "to run evals and track quality over time.\n",
       "- Production monitoring: Dashboards for quality, latency, cost, token usage, and error rates; slice by tag, \n",
       "version, user cohort, or route. Set up monitors and alerts to catch regressions.\n",
       "- Integrations: First-class with LangChain/LangGraph; works with any stack via REST/SDK; supports \n",
       "OpenTelemetry-style traces; connectors for major LLM providers. Exports/imports JSON for portability.\n",
       "\n",
       "Core concepts\n",
       "- Run/trace/span: A hierarchical record of an app execution <span style=\"font-weight: bold\">(</span>chain → tool → LLM call, etc.<span style=\"font-weight: bold\">)</span>.\n",
       "- Project/session: Logical grouping of runs <span style=\"font-weight: bold\">(</span>e.g., “staging”, “prod”<span style=\"font-weight: bold\">)</span>.\n",
       "- Dataset/example: Test collections for evaluations.\n",
       "- Feedback: Human or programmatic signals <span style=\"font-weight: bold\">(</span>thumbs up/down, scores, comments<span style=\"font-weight: bold\">)</span>.\n",
       "- Evaluator: A function <span style=\"font-weight: bold\">(</span>rule-based or LLM-based<span style=\"font-weight: bold\">)</span> that scores outputs.\n",
       "\n",
       "Typical workflow\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span> Instrument your app to send traces. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> Create a dataset of representative inputs and expected outputs \n",
       "<span style=\"font-weight: bold\">(</span>optional<span style=\"font-weight: bold\">)</span>. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span> Iterate on prompts, models, or routing. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span> Run evaluations and compare variants. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span> Ship and monitor\n",
       "in production; keep running evals to prevent regressions.\n",
       "\n",
       "Quick start\n",
       "- Sign up at LangSmith, create a project, get an API key.\n",
       "- Install SDK <span style=\"font-weight: bold\">(</span>Python or JS<span style=\"font-weight: bold\">)</span> and enable tracing via env vars or client config.\n",
       "- Wrap your LLM calls/chains; traces appear in the LangSmith UI.\n",
       "- Create a dataset and run evaluations to baseline quality.\n",
       "- Use experiments to compare model/prompt/routing changes before deploying.\n",
       "\n",
       "When to use it\n",
       "- You have multi-step LLM apps <span style=\"font-weight: bold\">(</span>RAG, agents, tools<span style=\"font-weight: bold\">)</span> and need visibility and quality control.\n",
       "- You want reproducible evals and guardrail checks before/after releases.\n",
       "- You need production analytics and alerts for latency/cost/errors/quality.\n",
       "\n",
       "Notes\n",
       "- Works with or without LangChain; LangChain/LangGraph just offer zero-friction tracing.\n",
       "- Pricing typically includes a free tier plus usage-based/enterprise options; check the site for current details \n",
       "and data governance features.\n",
       "\n",
       "If you share your stack <span style=\"font-weight: bold\">(</span>Python/JS, frameworks, models<span style=\"font-weight: bold\">)</span> and goals <span style=\"font-weight: bold\">(</span>debugging, RAG quality, A/B testing, production \n",
       "monitoring<span style=\"font-weight: bold\">)</span>, I can suggest a minimal setup and example instrumentation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LangSmith \u001b[1m(\u001b[0mby LangChain\u001b[1m)\u001b[0m is an LLM app development platform for tracing, testing, evaluating, and monitoring AI \n",
       "chains/agents in both development and production. Think of it as observability + experiment tracking + evaluation \n",
       "for LLM systems, model/framework agnostic, with SDKs for Python and JavaScript.\n",
       "\n",
       "What it does\n",
       "- Tracing and observability: Capture detailed, hierarchical traces of runs \u001b[1m(\u001b[0mchains, tools, model calls, retrievers\u001b[1m)\u001b[0m\n",
       "with inputs/outputs, tokens, latency, errors, and metadata. Useful for debugging complex LangChain/LangGraph \n",
       "pipelines and any custom code.\n",
       "- Evaluation and testing: Run offline or continuous evaluations with heuristic checks \u001b[1m(\u001b[0mexact match, regex\u001b[1m)\u001b[0m, \n",
       "semantic metrics \u001b[1m(\u001b[0msimilarity, correctness\u001b[1m)\u001b[0m, and LLM-judge–based graders \u001b[1m(\u001b[0me.g., faithfulness for RAG\u001b[1m)\u001b[0m. Supports \n",
       "dataset-based regression tests and pairwise comparisons across model/prompt variants.\n",
       "- Prompt and experiment management: Version prompts, compare variants, and record experiments with automatic \n",
       "lineage, tags, and metrics.\n",
       "- Datasets and annotation: Create datasets of inputs/expected outputs, collect human feedback/labels, and use them \n",
       "to run evals and track quality over time.\n",
       "- Production monitoring: Dashboards for quality, latency, cost, token usage, and error rates; slice by tag, \n",
       "version, user cohort, or route. Set up monitors and alerts to catch regressions.\n",
       "- Integrations: First-class with LangChain/LangGraph; works with any stack via REST/SDK; supports \n",
       "OpenTelemetry-style traces; connectors for major LLM providers. Exports/imports JSON for portability.\n",
       "\n",
       "Core concepts\n",
       "- Run/trace/span: A hierarchical record of an app execution \u001b[1m(\u001b[0mchain → tool → LLM call, etc.\u001b[1m)\u001b[0m.\n",
       "- Project/session: Logical grouping of runs \u001b[1m(\u001b[0me.g., “staging”, “prod”\u001b[1m)\u001b[0m.\n",
       "- Dataset/example: Test collections for evaluations.\n",
       "- Feedback: Human or programmatic signals \u001b[1m(\u001b[0mthumbs up/down, scores, comments\u001b[1m)\u001b[0m.\n",
       "- Evaluator: A function \u001b[1m(\u001b[0mrule-based or LLM-based\u001b[1m)\u001b[0m that scores outputs.\n",
       "\n",
       "Typical workflow\n",
       "\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m Instrument your app to send traces. \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m Create a dataset of representative inputs and expected outputs \n",
       "\u001b[1m(\u001b[0moptional\u001b[1m)\u001b[0m. \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m Iterate on prompts, models, or routing. \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m Run evaluations and compare variants. \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m Ship and monitor\n",
       "in production; keep running evals to prevent regressions.\n",
       "\n",
       "Quick start\n",
       "- Sign up at LangSmith, create a project, get an API key.\n",
       "- Install SDK \u001b[1m(\u001b[0mPython or JS\u001b[1m)\u001b[0m and enable tracing via env vars or client config.\n",
       "- Wrap your LLM calls/chains; traces appear in the LangSmith UI.\n",
       "- Create a dataset and run evaluations to baseline quality.\n",
       "- Use experiments to compare model/prompt/routing changes before deploying.\n",
       "\n",
       "When to use it\n",
       "- You have multi-step LLM apps \u001b[1m(\u001b[0mRAG, agents, tools\u001b[1m)\u001b[0m and need visibility and quality control.\n",
       "- You want reproducible evals and guardrail checks before/after releases.\n",
       "- You need production analytics and alerts for latency/cost/errors/quality.\n",
       "\n",
       "Notes\n",
       "- Works with or without LangChain; LangChain/LangGraph just offer zero-friction tracing.\n",
       "- Pricing typically includes a free tier plus usage-based/enterprise options; check the site for current details \n",
       "and data governance features.\n",
       "\n",
       "If you share your stack \u001b[1m(\u001b[0mPython/JS, frameworks, models\u001b[1m)\u001b[0m and goals \u001b[1m(\u001b[0mdebugging, RAG quality, A/B testing, production \n",
       "monitoring\u001b[1m)\u001b[0m, I can suggest a minimal setup and example instrumentation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response_openAI.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931b2ba",
   "metadata": {},
   "source": [
    "#### Output parser \n",
    "1. stroutput parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418aa53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain_outputParser=prompt|llm|output_parser # This is the workflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3bd8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output=chain_outputParser.invoke({\"input\":\"can you tell me about Langsmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2af74f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">## LangSmith – the “debugging‑and‑monitoring‑suite” for LLM apps\n",
       "\n",
       "| Feature | What it does | Why it matters |\n",
       "|---------|--------------|----------------|\n",
       "| **Tracing &amp; logging** | Every call to an LLM, prompt, tool, or chain is automatically logged as a *trace*. | See \n",
       "the full call stack, prompt templates, tool outputs, and LLM responses in a single, searchable view. |\n",
       "| **Metrics &amp; analytics** | Built‑in dashboards for latency, token usage, cost, and error rates. | Quickly spot \n",
       "bottlenecks, cost spikes, or quality regressions. |\n",
       "| **Evaluation &amp; benchmarking** | Run automated tests <span style=\"font-weight: bold\">(</span>e.g., correctness, hallucination rate, safety<span style=\"font-weight: bold\">)</span> and compare \n",
       "multiple models or prompts side‑by‑side. | Quantify the impact of changes and enforce SLAs. |\n",
       "| **Compliance &amp; audit** | Keep a tamper‑proof record of every inference. | Meets regulatory needs <span style=\"font-weight: bold\">(</span>GDPR, HIPAA, \n",
       "etc.<span style=\"font-weight: bold\">)</span> and supports internal audits. |\n",
       "| **Observability hooks** | OpenTelemetry‑compatible instrumentation for custom frameworks. | Plug into your own \n",
       "telemetry stack if you prefer CloudWatch, Prometheus, etc. |\n",
       "| **Open‑source core + hosted SaaS** | The core library is MIT‑licensed; LangSmith’s hosted UI is free for \n",
       "open‑source projects and paid for enterprise. | You can self‑host or use the cloud‑managed UI/analytics. |\n",
       "\n",
       "---\n",
       "\n",
       "### Quick start\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Install the SDK**\n",
       "\n",
       "   ```bash\n",
       "   pip install langsmith\n",
       "   ```\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Set up your API key <span style=\"color: #800080; text-decoration-color: #800080\">/</span> endpoint**\n",
       "\n",
       "   ```bash\n",
       "   export <span style=\"color: #808000; text-decoration-color: #808000\">LANGCHAIN_ENDPOINT</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"https://api.smith.langchain.com\"</span>\n",
       "   export <span style=\"color: #808000; text-decoration-color: #808000\">LANGCHAIN_API_KEY</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"YOUR_API_KEY\"</span>\n",
       "   ```\n",
       "\n",
       "   *If you’re self‑hosting, point `LANGCHAIN_ENDPOINT` to your own instance.*\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Wrap your LLM or chain**\n",
       "\n",
       "   ```python\n",
       "   from langchain import PromptTemplate, LLMChain\n",
       "   from langchain.llms import OpenAI\n",
       "   import os\n",
       "\n",
       "   os.environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"LANGCHAIN_ENDPOINT\"</span><span style=\"font-weight: bold\">]</span> = <span style=\"color: #008000; text-decoration-color: #008000\">\"https://api.smith.langchain.com\"</span>\n",
       "   os.environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"LANGCHAIN_API_KEY\"</span><span style=\"font-weight: bold\">]</span> = <span style=\"color: #008000; text-decoration-color: #008000\">\"YOUR_API_KEY\"</span>\n",
       "\n",
       "   llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAI</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">)</span>\n",
       "   template = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"question\"</span><span style=\"font-weight: bold\">]</span>,\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Answer this question: {question}\"</span>\n",
       "   <span style=\"font-weight: bold\">)</span>\n",
       "   chain = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMChain</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">llm</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080\">template</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "   response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chain.run</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"question\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"What is LangSmith?\"</span><span style=\"font-weight: bold\">})</span>\n",
       "   <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>response<span style=\"font-weight: bold\">)</span>\n",
       "   ```\n",
       "\n",
       "   *No extra tracing code needed – the SDK auto‑injects a trace.*\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Explore the UI**\n",
       "\n",
       "   - Go to your LangSmith dashboard → “Traces” to see the call chain, prompt, and response.\n",
       "   - “Metrics” shows latency, token usage, and cost over time.\n",
       "   - “Evaluation” lets you upload a CSV of prompts and expected answers and run a test suite.\n",
       "\n",
       "---\n",
       "\n",
       "### How it fits into the LangChain ecosystem\n",
       "\n",
       "| Tool | Integration |\n",
       "|------|-------------|\n",
       "| **LangChain** | Built‑in, no extra plumbing. |\n",
       "| **LlamaIndex** | Supports LangSmith tracing via the same `langsmith` hooks. |\n",
       "| **LangChain Hub** | You can publish a chain with trace metadata that’s instantly viewable on LangSmith. |\n",
       "| **OpenAI, Anthropic, Azure OpenAI, etc.** | All supported via the standard LangChain LLM adapters. |\n",
       "\n",
       "---\n",
       "\n",
       "### Common use‑cases\n",
       "\n",
       "| Scenario | How LangSmith helps |\n",
       "|----------|---------------------|\n",
       "| **Debugging a hallucination** | Open the trace, see the prompt that caused the hallucination, tweak it, and \n",
       "re‑run. |\n",
       "| **Cost control** | Identify the most expensive prompts or calls; tweak temperature or token limits. |\n",
       "| **Performance regression** | Run nightly benchmarks; LangSmith’s evaluation UI flags when latency or accuracy \n",
       "drops. |\n",
       "| **Compliance audit** | Export all traces; they contain request payloads, responses, and timestamps. |\n",
       "| **Rapid prototyping** | Spin up a LangSmith instance, write a chain, see results instantly—no custom logging \n",
       "infrastructure needed. |\n",
       "\n",
       "---\n",
       "\n",
       "### Advanced Topics\n",
       "\n",
       "| Feature | How to use it |\n",
       "|---------|---------------|\n",
       "| **Custom instrumentation** | Use the `langsmith` OpenTelemetry exporter to push traces to your own OTLP \n",
       "collector. |\n",
       "| **Batch evaluation** | `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">langsmith.evaluate.run</span><span style=\"font-weight: bold\">()</span>` can run thousands of prompts and aggregate metrics. |\n",
       "| **Custom dashboards** | Export trace data via the API <span style=\"font-weight: bold\">(</span>`langsmith.get_traces`<span style=\"font-weight: bold\">)</span> and feed it into Grafana, PowerBI,\n",
       "etc. |\n",
       "| **Fine‑tuning monitoring** | Log each fine‑tune job, track training metrics, and compare model versions. |\n",
       "\n",
       "---\n",
       "\n",
       "### Getting help &amp; resources\n",
       "\n",
       "| Resource | Link |\n",
       "|----------|------|\n",
       "| Official docs | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.smith.langchain.com</span> |\n",
       "| SDK repo | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/langchain-ai/langsmith</span> |\n",
       "| Community Discord | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discord.gg/langchain</span> |\n",
       "| Blog post: “Building a trace‑enabled LLM app” | <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://blog.langchain.com/tracing-llm-apps</span> |\n",
       "\n",
       "---\n",
       "\n",
       "### Bottom line\n",
       "\n",
       "LangSmith turns the opaque “black‑box” of LLM inference into a visible, measurable, and auditable workflow. Whether\n",
       "you’re a hobbyist prototyping a chatbot or a data‑science team shipping production‑grade AI, LangSmith gives you \n",
       "the observability you need without adding boilerplate code. Try it out today and see your LLM calls go from mystery\n",
       "to a fully‑instrumented data pipeline.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "## LangSmith – the “debugging‑and‑monitoring‑suite” for LLM apps\n",
       "\n",
       "| Feature | What it does | Why it matters |\n",
       "|---------|--------------|----------------|\n",
       "| **Tracing & logging** | Every call to an LLM, prompt, tool, or chain is automatically logged as a *trace*. | See \n",
       "the full call stack, prompt templates, tool outputs, and LLM responses in a single, searchable view. |\n",
       "| **Metrics & analytics** | Built‑in dashboards for latency, token usage, cost, and error rates. | Quickly spot \n",
       "bottlenecks, cost spikes, or quality regressions. |\n",
       "| **Evaluation & benchmarking** | Run automated tests \u001b[1m(\u001b[0me.g., correctness, hallucination rate, safety\u001b[1m)\u001b[0m and compare \n",
       "multiple models or prompts side‑by‑side. | Quantify the impact of changes and enforce SLAs. |\n",
       "| **Compliance & audit** | Keep a tamper‑proof record of every inference. | Meets regulatory needs \u001b[1m(\u001b[0mGDPR, HIPAA, \n",
       "etc.\u001b[1m)\u001b[0m and supports internal audits. |\n",
       "| **Observability hooks** | OpenTelemetry‑compatible instrumentation for custom frameworks. | Plug into your own \n",
       "telemetry stack if you prefer CloudWatch, Prometheus, etc. |\n",
       "| **Open‑source core + hosted SaaS** | The core library is MIT‑licensed; LangSmith’s hosted UI is free for \n",
       "open‑source projects and paid for enterprise. | You can self‑host or use the cloud‑managed UI/analytics. |\n",
       "\n",
       "---\n",
       "\n",
       "### Quick start\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Install the SDK**\n",
       "\n",
       "   ```bash\n",
       "   pip install langsmith\n",
       "   ```\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Set up your API key \u001b[35m/\u001b[0m endpoint**\n",
       "\n",
       "   ```bash\n",
       "   export \u001b[33mLANGCHAIN_ENDPOINT\u001b[0m=\u001b[32m\"https\u001b[0m\u001b[32m://api.smith.langchain.com\"\u001b[0m\n",
       "   export \u001b[33mLANGCHAIN_API_KEY\u001b[0m=\u001b[32m\"YOUR_API_KEY\"\u001b[0m\n",
       "   ```\n",
       "\n",
       "   *If you’re self‑hosting, point `LANGCHAIN_ENDPOINT` to your own instance.*\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Wrap your LLM or chain**\n",
       "\n",
       "   ```python\n",
       "   from langchain import PromptTemplate, LLMChain\n",
       "   from langchain.llms import OpenAI\n",
       "   import os\n",
       "\n",
       "   os.environ\u001b[1m[\u001b[0m\u001b[32m\"LANGCHAIN_ENDPOINT\"\u001b[0m\u001b[1m]\u001b[0m = \u001b[32m\"https://api.smith.langchain.com\"\u001b[0m\n",
       "   os.environ\u001b[1m[\u001b[0m\u001b[32m\"LANGCHAIN_API_KEY\"\u001b[0m\u001b[1m]\u001b[0m = \u001b[32m\"YOUR_API_KEY\"\u001b[0m\n",
       "\n",
       "   llm = \u001b[1;35mOpenAI\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.7\u001b[0m\u001b[1m)\u001b[0m\n",
       "   template = \u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "       \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"question\"\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mtemplate\u001b[0m=\u001b[32m\"Answer\u001b[0m\u001b[32m this question: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\n",
       "   \u001b[1m)\u001b[0m\n",
       "   chain = \u001b[1;35mLLMChain\u001b[0m\u001b[1m(\u001b[0m\u001b[33mllm\u001b[0m=\u001b[35mllm\u001b[0m, \u001b[33mprompt\u001b[0m=\u001b[35mtemplate\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "   response = \u001b[1;35mchain.run\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"question\"\u001b[0m: \u001b[32m\"What is LangSmith?\"\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "   \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mresponse\u001b[1m)\u001b[0m\n",
       "   ```\n",
       "\n",
       "   *No extra tracing code needed – the SDK auto‑injects a trace.*\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Explore the UI**\n",
       "\n",
       "   - Go to your LangSmith dashboard → “Traces” to see the call chain, prompt, and response.\n",
       "   - “Metrics” shows latency, token usage, and cost over time.\n",
       "   - “Evaluation” lets you upload a CSV of prompts and expected answers and run a test suite.\n",
       "\n",
       "---\n",
       "\n",
       "### How it fits into the LangChain ecosystem\n",
       "\n",
       "| Tool | Integration |\n",
       "|------|-------------|\n",
       "| **LangChain** | Built‑in, no extra plumbing. |\n",
       "| **LlamaIndex** | Supports LangSmith tracing via the same `langsmith` hooks. |\n",
       "| **LangChain Hub** | You can publish a chain with trace metadata that’s instantly viewable on LangSmith. |\n",
       "| **OpenAI, Anthropic, Azure OpenAI, etc.** | All supported via the standard LangChain LLM adapters. |\n",
       "\n",
       "---\n",
       "\n",
       "### Common use‑cases\n",
       "\n",
       "| Scenario | How LangSmith helps |\n",
       "|----------|---------------------|\n",
       "| **Debugging a hallucination** | Open the trace, see the prompt that caused the hallucination, tweak it, and \n",
       "re‑run. |\n",
       "| **Cost control** | Identify the most expensive prompts or calls; tweak temperature or token limits. |\n",
       "| **Performance regression** | Run nightly benchmarks; LangSmith’s evaluation UI flags when latency or accuracy \n",
       "drops. |\n",
       "| **Compliance audit** | Export all traces; they contain request payloads, responses, and timestamps. |\n",
       "| **Rapid prototyping** | Spin up a LangSmith instance, write a chain, see results instantly—no custom logging \n",
       "infrastructure needed. |\n",
       "\n",
       "---\n",
       "\n",
       "### Advanced Topics\n",
       "\n",
       "| Feature | How to use it |\n",
       "|---------|---------------|\n",
       "| **Custom instrumentation** | Use the `langsmith` OpenTelemetry exporter to push traces to your own OTLP \n",
       "collector. |\n",
       "| **Batch evaluation** | `\u001b[1;35mlangsmith.evaluate.run\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` can run thousands of prompts and aggregate metrics. |\n",
       "| **Custom dashboards** | Export trace data via the API \u001b[1m(\u001b[0m`langsmith.get_traces`\u001b[1m)\u001b[0m and feed it into Grafana, PowerBI,\n",
       "etc. |\n",
       "| **Fine‑tuning monitoring** | Log each fine‑tune job, track training metrics, and compare model versions. |\n",
       "\n",
       "---\n",
       "\n",
       "### Getting help & resources\n",
       "\n",
       "| Resource | Link |\n",
       "|----------|------|\n",
       "| Official docs | \u001b[4;94mhttps://docs.smith.langchain.com\u001b[0m |\n",
       "| SDK repo | \u001b[4;94mhttps://github.com/langchain-ai/langsmith\u001b[0m |\n",
       "| Community Discord | \u001b[4;94mhttps://discord.gg/langchain\u001b[0m |\n",
       "| Blog post: “Building a trace‑enabled LLM app” | \u001b[4;94mhttps://blog.langchain.com/tracing-llm-apps\u001b[0m |\n",
       "\n",
       "---\n",
       "\n",
       "### Bottom line\n",
       "\n",
       "LangSmith turns the opaque “black‑box” of LLM inference into a visible, measurable, and auditable workflow. Whether\n",
       "you’re a hobbyist prototyping a chatbot or a data‑science team shipping production‑grade AI, LangSmith gives you \n",
       "the observability you need without adding boilerplate code. Try it out today and see your LLM calls go from mystery\n",
       "to a fully‑instrumented data pipeline.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(result_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341caba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
